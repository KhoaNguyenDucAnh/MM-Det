import json
import logging
import os
import random
import sys
from copy import deepcopy

import numpy as np
import torch
import zarr
from lightning.pytorch.callbacks import BasePredictionWriter


def set_random_seed(seed):
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)


def construct_cached_mm_representations(args):
    root = args["mm_root"]
    cached_mm_representations = {}
    for dname in args["datasets"]:
        labels = []
        if "selected_cls_labels" in args["datasets"][dname]:  # only load partially
            for ele in args["datasets"][dname]["selected_cls_labels"]:
                labels.append(ele[0])
        else:
            labels = sorted(os.listdir(os.path.join(args["data_root"], dname)))
            labels = filter(
                lambda label: os.path.isdir(
                    os.path.join(args["data_root"], dname, label)
                ),
                labels,
            )
        cached_mm_representations[dname] = {}
        for label in labels:
            if os.path.exists(os.path.join(args["mm_root"], dname, label)):
                mm_representation = torch.load(
                    os.path.join(args["mm_root"], dname, label, "mm_representation.pth")
                )
                cached_mm_representations[dname][label] = mm_representation
    # build index table for search
    cached_mm_representations_index = {}
    for dname in cached_mm_representations:
        cached_mm_representations_index[dname] = {}
        for label in cached_mm_representations[dname]:
            cached_mm_representations_index[dname][label] = {}
            for frame_id in cached_mm_representations[dname][label]:
                base_id = os.path.splitext(frame_id)[0]
                prefix, index = base_id.rsplit("_", maxsplit=1)
                if prefix not in cached_mm_representations_index[dname][label]:
                    cached_mm_representations_index[dname][label][prefix] = [int(index)]
                else:
                    cached_mm_representations_index[dname][label][prefix].append(
                        int(index)
                    )

    return cached_mm_representations, cached_mm_representations_index


def get_nearest_mm_index(cached_index, index):
    index = int(index)
    previous_index = []
    for i in cached_index:
        if i <= index:
            previous_index.append(i)
    return max(previous_index)


def get_logger(name, config):
    logger = logging.getLogger(name)
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.flush = sys.stdout.flush
    stream_handler.setLevel(level=logging.INFO)
    logger.addHandler(stream_handler)
    os.makedirs(os.path.join("expts", config["expt"]), exist_ok=True)
    out_handler = logging.FileHandler(
        filename=os.path.join("expts", config["expt"], f'{config["mode"]}.log')
    )
    out_handler.setLevel(level=logging.INFO)
    logger.addHandler(out_handler)
    logger.setLevel(logging.INFO)
    return logger


def get_train_dataset_config(config):
    dataset_classes = config["classes"]
    dataset_config = {}
    for dataset_class in dataset_classes:
        dataset_config[dataset_class] = {
            "data_root": f'{config["data_root"]}/{dataset_class}',
            "dataset_type": "VideoFolderDatasetForReconsWithFn",
            "mode": config["mode"],
            "selected_cls_labels": [("0_real", 0), ("1_fake", 1)],
        }
        if config["fix_split"] and config["mode"] == "train":
            dataset_config[dataset_class]["split"] = {
                "train": json.load(
                    open(os.path.join(config["split_path"], "train.json"))
                )[dataset_class],
                "val": json.load(open(os.path.join(config["split_path"], "val.json")))[
                    dataset_class
                ],
            }
    return dataset_config


def get_test_dataset_configs(config):
    dataset_classes = config["classes"]
    dataset_configs = []
    default_real_dataset_config = {}
    default_real_test_datasets_classes = ["videocrafter1", "zeroscope", "opensora"]
    for (
        real_dataset_class
    ) in default_real_test_datasets_classes:  # load default real dataset
        default_real_dataset_config[real_dataset_class] = {
            "data_root": f'{config["data_root"]}/{real_dataset_class}',
            "dataset_type": "VideoFolderDatasetForReconsWithFn",
            "mode": config["mode"],
            "selected_cls_labels": [("0_real", 0)],
            "sample_method": "entire",
        }
    for dataset_class in dataset_classes:  # load fake datasets
        dataset_config = deepcopy(default_real_dataset_config)
        if dataset_class not in dataset_config:
            dataset_config[dataset_class] = {
                "data_root": f'{config["data_root"]}/{dataset_class}',
                "dataset_type": "VideoFolderDatasetForReconsWithFn",
                "mode": config["mode"],
                "selected_cls_labels": [("1_fake", 1)],
                "sample_method": "entire",
            }
        else:
            dataset_config[dataset_class]["selected_cls_labels"].append(("1_fake", 1))
        dataset_configs.append(dataset_config)
    return dataset_configs


class CustomWriter(BasePredictionWriter):
    def __init__(self, output_file):
        super().__init__("batch")
        self.output_file = output_file

    def write_on_batch_end(
        self,
        trainer,
        pl_module,
        prediction,
        batch_indices,
        batch,
        batch_idx,
        dataloader_idx,
    ):
        if prediction == None:
            return
        file = zarr.open_group(self.output_file, mode="a")
        for key, value in prediction.items():
            file.array(
                name=key,
                data=value,
                shape=value.shape,
                dtype=value.dtype,
                overwrite=True,
            )
